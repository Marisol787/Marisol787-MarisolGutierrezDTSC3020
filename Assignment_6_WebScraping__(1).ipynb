{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marisol787/Marisol787-MarisolGutierrezDTSC3020/blob/main/Assignment_6_WebScraping__(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_de5Eq4u-tR"
      },
      "source": [
        "# Assignment 6 (4 points) — Web Scraping\n",
        "\n",
        "In this assignment you will complete **two questions**. The **deadline is posted on Canvas**.\n"
      ],
      "id": "H_de5Eq4u-tR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PHwamZMu-tX"
      },
      "source": [
        "## Assignment Guide (Read Me First)\n",
        "\n",
        "- This notebook provides an **Install Required Libraries** cell and a **Common Imports & Polite Headers** cell. Run them first.\n",
        "- Each question includes a **skeleton**. The skeleton is **not** a solution; it is a lightweight scaffold you may reuse.\n",
        "- Under each skeleton you will find a **“Write your answer here”** code cell. Implement your scraping, cleaning, and saving logic there.\n",
        "- When your code is complete, run the **Runner** cell to print a Top‑15 preview and save the CSV.\n",
        "- Expected outputs:\n",
        "  - **Q1:** `data_q1.csv` + Top‑15 sorted by the specified numeric column.\n",
        "  - **Q2:** `data_q2.csv` + Top‑15 sorted by `points`.\n"
      ],
      "id": "4PHwamZMu-tX"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I7DLq9nEu-tZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "519b3268-8877-411d-9d9e-0266974324fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "# Install Required Libraries\n",
        "!pip -q install requests beautifulsoup4 lxml pandas\n",
        "print(\"Dependencies installed.\")"
      ],
      "id": "I7DLq9nEu-tZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug_A9RuPu-tb"
      },
      "source": [
        "### 2) Common Imports & Polite Headers"
      ],
      "id": "ug_A9RuPu-tb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ov8pXh65u-tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023fee2a-f468-4db2-d5aa-58507dcb83b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common helpers loaded.\n"
          ]
        }
      ],
      "source": [
        "# Common Imports & Polite Headers\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "print(\"Common helpers loaded.\")\n"
      ],
      "id": "Ov8pXh65u-tc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km0GO7zzu-td"
      },
      "source": [
        "## Question 1 — IBAN Country Codes (table)\n",
        "**URL:** https://www.iban.com/country-codes  \n",
        "**Extract at least:** `Country`, `Alpha-2`, `Alpha-3`, `Numeric` (≥4 cols; you may add more)  \n",
        "**Clean:** trim spaces; `Alpha-2/Alpha-3` → **UPPERCASE**; `Numeric` → **int** (nullable OK)  \n",
        "**Output:** write **`data_q1.csv`** and **print a Top-15** sorted by `Numeric` (desc, no charts)  \n",
        "**Deliverables:** notebook + `data_q1.csv` + short `README.md` (URL, steps, 1 limitation)\n",
        "\n",
        "**Tip:** You can use `pandas.read_html(html)` to read tables and then pick one with ≥3 columns.\n"
      ],
      "id": "km0GO7zzu-td"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "q1_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q1 Skeleton (fill the TODOs) ---\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return the first table with >= 3 columns from the HTML.\n",
        "    TODO: implement with pd.read_html(html), pick a reasonable table, then flatten headers.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_read_table\")\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean columns: strip, UPPER Alpha-2/Alpha-3, cast Numeric to int (nullable), drop invalids.\n",
        "    TODO: implement cleaning steps.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_clean\")\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort descending by Numeric and return Top-N.\n",
        "    TODO: implement.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q1_sort_top\")\n"
      ],
      "id": "q1_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "q1_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28d2ce0-cbe4-4619-e779-405d337bf9c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP 15 IBAN Country Codes!!\n",
            "                                                   Country Alpha-2 Code Alpha-3 Code  Numeric\n",
            "                                                    ZAMBIA           ZM          ZMB      894\n",
            "                                                     YEMEN           YE          YEM      887\n",
            "                                                     SAMOA           WS          WSM      882\n",
            "                                         WALLIS AND FUTUNA           WF          WLF      876\n",
            "                        VENEZUELA (BOLIVARIAN REPUBLIC OF)           VE          VEN      862\n",
            "                                                UZBEKISTAN           UZ          UZB      860\n",
            "                                                   URUGUAY           UY          URY      858\n",
            "                                              BURKINA FASO           BF          BFA      854\n",
            "                                     VIRGIN ISLANDS (U.S.)           VI          VIR      850\n",
            "                            UNITED STATES OF AMERICA (THE)           US          USA      840\n",
            "                              TANZANIA, UNITED REPUBLIC OF           TZ          TZA      834\n",
            "                                               ISLE OF MAN           IM          IMN      833\n",
            "                                                    JERSEY           JE          JEY      832\n",
            "                                                  GUERNSEY           GG          GGY      831\n",
            "UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND (THE)           GB          GBR      826\n",
            "\n",
            "Saved as data_q1.csv\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3888476293.py:25: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  tables = pd.read_html(html)\n"
          ]
        }
      ],
      "source": [
        "# Q1 — Write your answer here\n",
        "\n",
        "import re, sys, pandas as pd, requests\n",
        "from bs4 import BeautifulSoup\n",
        "HEADERS = {\"User-Agent\": (\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
        "    \"(KHTML, like Gecko) Chrome/122.0 Safari/537.36\")}\n",
        "\n",
        "def fetch_html(url: str, timeout: int = 20) -> str:\n",
        "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "def flatten_headers(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df.columns = [\" \".join([str(x) for x in tup if str(x)!=\"nan\"]).strip()\n",
        "                      for tup in df.columns.values]\n",
        "    else:\n",
        "        df.columns = [str(c).strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "Q1LINK =\"https://www.iban.com/country-codes\"\n",
        "\n",
        "def q1_read_table(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Return first table with >=3 columns using pandas.read_html.\"\"\"\n",
        "    tables = pd.read_html(html)\n",
        "    for t in tables:\n",
        "        if t.shape[1] >= 3:\n",
        "            df = flatten_headers(t)\n",
        "            return df\n",
        "    raise ValueError(\"no valid table found with >=3 columns.\")\n",
        "\n",
        "\n",
        "def q1_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df.columns = [c.strip().title() for c in df.columns]\n",
        "    df = df.rename(columns={\n",
        "        \"Alpha-2 code\":\"Alpha-2\",\n",
        "        \"Alpha-3 code\":\"Alpha-3\",\n",
        "        \"Numeric code\":\"Numeric\"\n",
        "    })\n",
        "    for col in [\"Country\", \"Alpha-2\", \"Alpha-3\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).str.strip().str.upper()\n",
        "    if \"Numeric\" in df.columns:\n",
        "        df[\"Numeric\"] = pd.to_numeric(df[\"Numeric\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    df = df.dropna(subset=[\"Country\"])\n",
        "    return df\n",
        "\n",
        "def q1_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "\n",
        "    return df.sort_values(by=\"Numeric\", ascending=False, na_position=\"last\").head(top)\n",
        "html1 = fetch_html(Q1LINK)\n",
        "df1 = q1_read_table(html1)\n",
        "df1 = q1_clean(df1)\n",
        "TOP15 = q1_sort_top(df1)\n",
        "print(\"TOP 15 IBAN Country Codes!!\")\n",
        "print(TOP15.to_string(index=False))\n",
        "df1.to_csv(\"data_q1.csv\", index=False)\n",
        "print(\"\\nSaved as data_q1.csv\\n\")\n",
        "\n",
        "\n"
      ],
      "id": "q1_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmefu--_u-tg"
      },
      "source": [
        "## Question 2 — Hacker News (front page)\n",
        "**URL:** https://news.ycombinator.com/  \n",
        "**Extract at least:** `rank`, `title`, `link`, `points`, `comments` (user optional)  \n",
        "**Clean:** cast `points`/`comments`/`rank` → **int** (non-digits → 0), fill missing text fields  \n",
        "**Output:** write **`data_q2.csv`** and **print a Top-15** sorted by `points` (desc, no charts)  \n",
        "**Tip:** Each story is a `.athing` row; details (points/comments/user) are in the next `<tr>` with `.subtext`.\n"
      ],
      "id": "rmefu--_u-tg"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "q2_skeleton"
      },
      "outputs": [],
      "source": [
        "# --- Q2 Skeleton (fill the TODOs) ---\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    TODO: implement with BeautifulSoup on '.athing' and its sibling '.subtext'.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_parse_items\")\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    TODO: cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_clean\")\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N. TODO: implement.\"\"\"\n",
        "    raise NotImplementedError(\"TODO: implement q2_sort_top\")\n"
      ],
      "id": "q2_skeleton"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "q2_skeleton_answer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20082e43-7173-4b46-cbbe-218a9ca48177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP 15 Hacker News \n",
            " rank                                                                       title                                                                                                                                                                link  points  comments         user\n",
            "   12 YouTube Removes Windows 11 Bypass Tutorials, Claims 'Risk of Physical Harm'                                                                                               https://news.itsfoss.com/youtube-removes-windows-11-bypass-tutorials/     398       148  WaitWaitWha\n",
            "   10                                                     Why I love OCaml (2023)                                                                                                                     https://mccd.space/posts/ocaml-the-worlds-best/     296       201        art-w\n",
            "   20              VLC's Jean-Baptiste Kempf Receives the European SFS Award 2025                                                                                                                 https://fsfe.org/news/2025/news-20251107-01.en.html     244        39    kirschner\n",
            "   24                                                       James Watson has died                                                                                                   https://www.nytimes.com/2025/11/07/science/james-watson-dead.html     233       127    granzymes\n",
            "    7    Myna: Monospace typeface designed for symbol-heavy programming languages                                                                                                                              https://github.com/sayyadirfanali/Myna     203        77  birdculture\n",
            "    9                                                      Ruby Solved My Problem                                                                                                   https://newsletter.masilotti.com/p/ruby-already-solved-my-problem     171        68 joemasilotti\n",
            "    1                                                         Why is Zig so cool?                                                                                                             https://nilostolte.github.io/tech/articles/ZigCool.html     149        48    vitalnodo\n",
            "    8                                                         How did I get here?                                                                                                                                     https://how-did-i-get-here.net/     120        33    zachlatta\n",
            "    3                                                Becoming a Compiler Engineer                                                                                                            https://rona.substack.com/p/becoming-a-compiler-engineer     111        49    lalitkale\n",
            "   23                                              Angel Investors, a Field Guide                                                                                                       https://www.jeanyang.com/posts/angel-investors-a-field-guide/     105        22     azhenley\n",
            "   15                                                     Venn Diagram for 7 Sets                                                                                                                              https://moebio.com/research/sevensets/      94        18  bramadityaw\n",
            "   17               FAA to restrict commercial rocket launches to overnight hours https://www.space.com/space-exploration/launches-spacecraft/faa-restricts-commercial-rocket-launches-indefinitely-due-to-air-traffic-risks-from-government-shutdown      90        32      bookmtn\n",
            "   13                    Transducer: Composition, abstraction, performance (2018)                                                                                                 https://funktionale-programmierung.de/en/2018/03/22/transducer.html      74         0     defmarco\n",
            "   30             I'm making a small RPG and I need feeback regarding performance                                                                                                 https://jslegenddev.substack.com/p/im-making-a-small-rpg-and-i-need      68        60       ibobev\n",
            "    2                   Snapchat open-sources Valdi a cross-platform UI framework                                                                                                                                   https://github.com/Snapchat/Valdi      66        17  yehiaabdelm\n",
            "\n",
            "Saved as data_q2.csv\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q2 — Write your answer here\n",
        "URL_Q2 = \"https://news.ycombinator.com/\"\n",
        "\n",
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Extract rank, title, link, points, comments, user from Hacker News.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    news = soup.select(\"tr.athing\")\n",
        "    rows = []\n",
        "    for s in news:\n",
        "        rank = s.select_one(\".rank\")\n",
        "        title = s.select_one(\".titleline a\")\n",
        "        subtext = s.find_next_sibling(\"tr\").select_one(\".subtext\")\n",
        "        points = subtext.select_one(\".score\") if subtext else None\n",
        "        user = subtext.select_one(\".hnuser\") if subtext else None\n",
        "        comments = None\n",
        "        if subtext:\n",
        "            cm = subtext.find_all(\"a\")\n",
        "            if cm:\n",
        "                for a in cm[::-1]:\n",
        "                    if \"comment\" in a.text:\n",
        "                        comments = a\n",
        "                        break\n",
        "        rows.append({\n",
        "            \"rank\": rank.text.replace(\".\", \"\").strip() if rank else \"\",\n",
        "            \"title\": title.text.strip() if title else \"\",\n",
        "            \"link\": title.get(\"href\") if title else \"\",\n",
        "            \"points\": points.text.strip() if points else \"\",\n",
        "            \"comments\": comments.text.strip() if comments else \"\",\n",
        "            \"user\": user.text.strip() if user else \"\",\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    for col in [\"rank\", \"points\", \"comments\"]:\n",
        "        df[col] = (\n",
        "            df[col].astype(str)\n",
        "            .str.extract(r\"(\\d+)\")[0]\n",
        "            .fillna(0)\n",
        "            .astype(int)\n",
        "        )\n",
        "    df = df.fillna(\"\")\n",
        "    return df\n",
        "\n",
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Return Top sorted by points desc.\"\"\"\n",
        "    return df.sort_values(by=\"points\", ascending=False).head(top)\n",
        "\n",
        "html2 = fetch_html(URL_Q2)\n",
        "df2 = q2_parse_items(html2)\n",
        "df2 = q2_clean(df2)\n",
        "top15_q2 = q2_sort_top(df2)\n",
        "\n",
        "print(\"TOP 15 Hacker News \")\n",
        "print(top15_q2.to_string(index=False))\n",
        "df2.to_csv(\"data_q2.csv\", index=False)\n",
        "print(\"\\nSaved as data_q2.csv\\n\")\n",
        "\n"
      ],
      "id": "q2_skeleton_answer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f356a2c9"
      },
      "source": [
        "# Task\n",
        "Implement the `q2_parse_items`, `q2_clean`, and `q2_sort_top` functions to extract `rank`, `title`, `link`, `points`, and `comments` from the Hacker News front page (\"https://news.ycombinator.com/\"), clean the data (casting numeric fields to int, handling non-digits as 0, and filling missing text fields), sort by `points` in descending order, print the top 15 results, and save the entire cleaned DataFrame to \"data_q2.csv\"."
      ],
      "id": "f356a2c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c319165"
      },
      "source": [
        "## Implement q2_parse_items\n",
        "\n",
        "### Subtask:\n",
        "Implement the `q2_parse_items` function to parse the Hacker News HTML, extracting the rank, title, link, points, and comments for each story into a pandas DataFrame. Use BeautifulSoup to navigate the HTML structure, specifically targeting `.athing` rows and their subsequent `.subtext` rows.\n"
      ],
      "id": "4c319165"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb954ad2"
      },
      "source": [
        "**Reasoning**:\n",
        "Implementing the `q2_parse_items` function as per the instructions, which involves parsing the HTML content using BeautifulSoup to extract specific data points (rank, title, link, points, comments, and optional user) and organizing them into a pandas DataFrame.\n",
        "\n"
      ],
      "id": "cb954ad2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daabe657"
      },
      "source": [
        "def q2_parse_items(html: str) -> pd.DataFrame:\n",
        "    \"\"\"Parse front page items into DataFrame columns:\n",
        "       rank, title, link, points, comments, user (optional).\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    athing_rows = soup.find_all(\"tr\", class_=\"athing\")\n",
        "\n",
        "    for row in athing_rows:\n",
        "        # Extract rank\n",
        "        rank_tag = row.find(\"span\", class_=\"rank\")\n",
        "        rank = int(rank_tag.get_text(strip=True).replace(\".\", \"\")) if rank_tag else None\n",
        "\n",
        "        # Extract title and link\n",
        "        title_line_tag = row.find(\"span\", class_=\"titleline\")\n",
        "        title_tag = title_line_tag.find(\"a\") if title_line_tag else None\n",
        "        title = title_tag.get_text(strip=True) if title_tag else None\n",
        "        link = title_tag.get(\"href\") if title_tag else None\n",
        "\n",
        "        # The subtext is typically in the next sibling tr\n",
        "        subtext_tr = row.find_next_sibling(\"tr\")\n",
        "        subtext_td = subtext_tr.find(\"td\", class_=\"subtext\") if subtext_tr else None\n",
        "\n",
        "        # Extract points\n",
        "        points = 0\n",
        "        score_tag = subtext_td.find(\"span\", class_=\"score\") if subtext_td else None\n",
        "        if score_tag:\n",
        "            points_text = score_tag.get_text(strip=True)\n",
        "            match = re.search(r'(\\d+)', points_text)\n",
        "            if match:\n",
        "                points = int(match.group(1))\n",
        "\n",
        "        # Extract comments\n",
        "        comments = 0\n",
        "        comment_tag = None\n",
        "        if subtext_td:\n",
        "            # Look for an anchor tag that contains 'comments', 'comment', or 'discuss'\n",
        "            # and also contains digits for the count.\n",
        "            for a_tag in subtext_td.find_all(\"a\"):\n",
        "                if re.search(r'comments?|discuss', a_tag.get_text(strip=True), re.IGNORECASE):\n",
        "                    comment_tag = a_tag\n",
        "                    break\n",
        "\n",
        "        if comment_tag:\n",
        "            comment_text = comment_tag.get_text(strip=True)\n",
        "            match = re.search(r'(\\d+)', comment_text)\n",
        "            if match:\n",
        "                comments = int(match.group(1))\n",
        "\n",
        "        # Extract user (optional)\n",
        "        user = None\n",
        "        user_tag = subtext_td.find(\"a\", class_=\"hnuser\") if subtext_td else None\n",
        "        if user_tag:\n",
        "            user = user_tag.get_text(strip=True)\n",
        "\n",
        "        data.append({\n",
        "            \"rank\": rank,\n",
        "            \"title\": title,\n",
        "            \"link\": link,\n",
        "            \"points\": points,\n",
        "            \"comments\": comments,\n",
        "            \"user\": user\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)"
      ],
      "id": "daabe657",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "088e737b"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to implement the `q2_clean` function according to the instructions, which involves casting numeric fields (`points`, `comments`, `rank`) to integers (treating non-digits as 0) and filling any missing text fields.\n",
        "\n"
      ],
      "id": "088e737b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "372af791"
      },
      "source": [
        "def q2_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Clean numeric fields and fill missing values.\n",
        "    Cast points/comments/rank to int (non-digits -> 0). Fill text fields.\n",
        "    \"\"\"\n",
        "    # Cast numeric fields to int, treating non-digits as 0\n",
        "    for col in [\"rank\", \"points\", \"comments\"]:\n",
        "        if col in df.columns:\n",
        "            # Convert to numeric, coerce errors to NaN, then fill NaN with 0 and cast to Int64\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0).astype('Int64')\n",
        "\n",
        "    # Fill missing text fields with appropriate defaults\n",
        "    for col in [\"title\", \"link\", \"user\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].fillna(\"N/A\")\n",
        "\n",
        "    return df"
      ],
      "id": "372af791",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d943cbd"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `q2_parse_items` and `q2_clean` are defined, the next step is to implement the `q2_sort_top` function as specified in the problem description, which involves sorting the DataFrame by `points` in descending order and returning the top N rows.\n",
        "\n"
      ],
      "id": "5d943cbd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a83d4735"
      },
      "source": [
        "def q2_sort_top(df: pd.DataFrame, top: int = 15) -> pd.DataFrame:\n",
        "    \"\"\"Sort by points desc and return Top-N.\"\"\"\n",
        "    return df.sort_values(by=\"points\", ascending=False).head(top)"
      ],
      "id": "a83d4735",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "713486eb"
      },
      "source": [
        "**Reasoning**:\n",
        "With all helper functions (`q2_parse_items`, `q2_clean`, and `q2_sort_top`) now defined, the next step is to execute the complete Question 2 pipeline: fetch the HTML from the specified URL, parse it using `q2_parse_items`, clean the resulting DataFrame using `q2_clean`, sort and select the top 15 entries using `q2_sort_top`, print the top 15, and save the full cleaned DataFrame to `data_q2.csv`.\n",
        "\n"
      ],
      "id": "713486eb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0f7e6ca",
        "outputId": "5535faf9-0218-48e2-d2d9-5f8dd79ac994"
      },
      "source": [
        "Q2LINK = \"https://news.ycombinator.com/\"\n",
        "\n",
        "html2 = fetch_html(Q2LINK)\n",
        "df2 = q2_parse_items(html2)\n",
        "df2 = q2_clean(df2)\n",
        "TOP15_Q2 = q2_sort_top(df2)\n",
        "\n",
        "print(\"Hacker News — Top 15 by Points\")\n",
        "print(TOP15_Q2[['rank', 'title', 'points', 'comments']].to_string(index=False))\n",
        "\n",
        "df2.to_csv(\"data_q2.csv\", index=False)\n",
        "print(\"\\nSaved as data_q2.csv\")"
      ],
      "id": "a0f7e6ca",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hacker News — Top 15 by Points\n",
            " rank                                                                         title  points  comments\n",
            "   29 Denmark's government aims to ban access to social media for children under 15     416       300\n",
            "   12   YouTube Removes Windows 11 Bypass Tutorials, Claims 'Risk of Physical Harm'     393       144\n",
            "   10                                                       Why I love OCaml (2023)     296       201\n",
            "   20                VLC's Jean-Baptiste Kempf Receives the European SFS Award 2025     240        39\n",
            "   21                                                         James Watson has died     231       125\n",
            "    7      Myna: Monospace typeface designed for symbol-heavy programming languages     202        77\n",
            "    9                                                        Ruby Solved My Problem     171        68\n",
            "    1                                                           Why is Zig so Cool?     144        44\n",
            "   30                                                                PyTorch Helion     131        43\n",
            "    8                                                           How did I get here?     119        33\n",
            "    3                                                  Becoming a Compiler Engineer     106        46\n",
            "   22                                                Angel Investors, a Field Guide     104        22\n",
            "   14                                                       Venn Diagram for 7 Sets      93        16\n",
            "   18                 FAA to restrict commercial rocket launches to overnight hours      85        29\n",
            "   13                      Transducer: Composition, abstraction, performance (2018)      73         0\n",
            "\n",
            "Saved as data_q2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ad1421"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the completion of Question 2, including the successful extraction, cleaning, and saving of Hacker News data, along with the displayed Top-15.\n"
      ],
      "id": "09ad1421"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8aa02db"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `q2_parse_items` function successfully extracted `rank`, `title`, `link`, `points`, `comments`, and `user` for each story from the Hacker News front page.\n",
        "*   The `q2_clean` function effectively cleaned the extracted data by casting numeric fields (`rank`, `points`, `comments`) to `Int64`, converting non-digit values to 0, and filling missing text fields (`title`, `link`, `user`) with \"N/A\".\n",
        "*   The `q2_sort_top` function correctly sorted the cleaned data by `points` in descending order and identified the top 15 stories.\n",
        "*   An example of a top story extracted is \"Denmark's government aims to ban access to social media for children under 15\" with 416 points and 300 comments.\n",
        "*   The entire cleaned DataFrame was successfully saved to \"data\\_q2.csv\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The implemented functions provide a reliable pipeline for periodically fetching, cleaning, and analyzing Hacker News data, which could be extended to track news trends or community engagement over time.\n",
        "*   To enhance the analysis, one could incorporate sentiment analysis on story titles or comments to gauge public opinion, or track the performance of specific authors/users.\n"
      ],
      "id": "e8aa02db"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}